#%%
#importando bibliotecas
import pandas as pd
import seaborn as sns
import catboost as cat
from catboost import CatBoostRegressor
from catboost import CatBoostClassifier  
from sklearn.metrics import confusion_matrix, classification_report 
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# %%
#importando base de dados
df_11 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2011.csv", sep=";")
df_12 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2012.csv", sep=";")
df_13 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2013.csv", sep=";")
df_14 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2014.csv", sep=";")
df_15 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2015.csv", sep=";")
df_16 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2016.csv", sep=";")
df_17 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2017.csv", sep=";")
df_18 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2018.csv", sep=";")
df_19 = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\RAIS_CTPS_CAGED_2019.csv", sep=";")

#importando dicionarios
df_muni = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\municipio_map.csv", sep=";")
df_cbo = pd.read_csv(r"C:\Users\ninja\OneDrive\Documentos\rct2\Awari\Trabalho Final\Brasil\Dados Fonte\CBO_map.csv", sep=";")


# %%
# adicionando inflação de cada DF e unificando
df_11 = df_11.assign(Ano=2011)
df_12 = df_12.assign(Ano=2012)
df_13 = df_13.assign(Ano=2013)
df_14 = df_14.assign(Ano=2014)
df_15 = df_15.assign(Ano=2015)
df_16 = df_16.assign(Ano=2016)
df_17 = df_17.assign(Ano=2017)
df_18 = df_18.assign(Ano=2018)
df_19 = df_19.assign(Ano=2019)

#inflação de anual
df_11 = df_11.assign(Inflacao=0.065)
df_12 = df_12.assign(Inflacao=0.0584)
df_13 = df_13.assign(Inflacao=0.0591)
df_14 = df_14.assign(Inflacao=0.0641)
df_15 = df_15.assign(Inflacao=0.1067)
df_16 = df_16.assign(Inflacao=0.0629)
df_17 = df_17.assign(Inflacao=0.0295)
df_18 = df_18.assign(Inflacao=0.0375)
df_19 = df_19.assign(Inflacao=0.0431)

# %%
#carregando maps

#Municipio-map
df_muni = df_muni.rename(columns = {'municipo':'municipio'})
df_muni = df_muni[[w.isnumeric() for w in df_muni["municipio"]]].reset_index(drop=True)
df_muni["municipio"] = df_muni["municipio"].astype(int)
muni_map = dict(df_muni.values) 

#CBO_2002-map
df_cbo = df_cbo.rename(columns = {'Classificação Brasileira de Ocupações, criada em 2002 - com atualizações':'classificacao'})
df_cbo = df_cbo[[w.isnumeric() for w in df_cbo["classificacao"]]].reset_index(drop=True)
df_cbo["classificacao"] = df_cbo["classificacao"].astype(int)
cbo_map = dict(df_cbo.values)

#Sexo-map
sexo_map = {1: "homem", 2: "mulher"}

#Continente map
conti_map = { "sia": "Asia", "frica": "Africa", "amrica central e caribe": "america central e caribe", "amrica do sul": "america do sul", "amrica do norte": "america do norte"}

#Raça-map
raca_map = {1:"indigena", 2:"branca", 4:"preta", 6:"amarela", 8:"parda", 9:"nao_ident", -1:"ignorado"}

# %%
# agrupando as base de dados
lista_concat = [df_11 , df_12 , df_13 , df_14, df_15 , df_16 , df_17 , df_18 , df_19 ]

concat_df = pd.concat(lista_concat)

# removendo nans 
# antes 1357088 rows
# depois 1184189  row
# %%
# Limpando DFs com coisas desnecessarias
def limpar_df(x):   
    x["cbo_2002"] = x["cbo_2002"].map(cbo_map)
    x["municipio"] = x["municipio"].map(muni_map)
    x["raca_cor"] = x["raca_cor"].map(raca_map)
    x["sexo"] = x["sexo"].map(sexo_map)   
    x["continente"] = x["continente"].str.normalize("NFKD").str.lower().str.encode("ascii", errors="ignore").str.decode("utf8")
    x["status_migratorio"] = x["status_migratorio"].str.normalize("NFKD").str.lower().str.encode("ascii", errors="ignore").str.decode("utf8")
    x["pais"] = x["pais"].str.normalize("NFKD").str.lower().str.encode("ascii", errors="ignore").str.decode("utf8")
    x["continente"] = x["continente"].map(conti_map)
    x = x.drop(columns=["cnae_20_subclas"])
    x = x.drop(columns=["movimento"])
    x = x.drop(columns=["cnae_20_classe"])
    x = x.drop(columns=["competencia"])
    x = x.drop(columns=["tipo_mov_desagregado"])
    x = x.drop(columns=["uf"]) 
    x = x.drop(columns=["indtrabintermitente"])
    x = x.drop(columns=["indtrabparcial"])
    x = x
    x = x.dropna()
    return x


# removendo nans 
# antes 1357088 rows
# depois 1184189  rows

#arrumando escrita e substituindo numeros por palavras

# %%
#Aplicando mudanças ao DF
concat_df = limpar_df(concat_df) 

df_machine = concat_df


# %%
#salvando pasta para Analise Exploratoria
#concat_df.to_csv("Analise_Tableau.csv", sep=";")

# %%
# Inicio Machine Learning
plt.figure(figsize=(10,10))
sns.heatmap(df_machine.corr(), annot=True)

# %%
#CatBoost Agrupamento
X = df_machine[[w for w in df_machine.columns if w!="nivel_instrucao"]]
y = df_machine["nivel_instrucao"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=12345)
# %%
categorical_features = ['pais', 'continente', 'municipio', 'cbo_2002', 'sexo', 
         'raca_cor', 'status_migratorio',]

cat = CatBoostClassifier(random_state=12345 , cat_features=categorical_features)

# %%
cat.fit(X_train, y_train, eval_set=(X_test, y_test))
y_pred = cat.predict(X_test)

# %%
print(classification_report(y_test, y_pred))

# %%
print(classification_report(y_train, cat.predict(X_train)))

# %%

cat.get_feature_importance(prettified=True)
